
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="High fidelity digital 3D environments have been proposed in recent years, however, it remains extremely challenging to automatically equip such environment with realistic human bodies. Existing work utilizes images, depth or semantic maps to represent the scene, and parametric human models to represent 3D bodies. While being straightforward, their generated human-scene interactions are often lack of naturalness and physical plausibility. Our key observation is that humans interact with the world through body-scene contact. To synthesize realistic human-scene interactions, it is essential to effectively represent the physical contact and proximity  between the body and the world. To that end, we propose a novel interaction generation method, named PLACE (Proximity Learning of Articulation and Contact in 3D Environments), which explicitly models the proximity between the human body and the 3D scene around it. Specifically, given a set of basis points on a scene mesh, we leverage a conditional variational autoencoder to synthesize the minimum distances from the basis points to the human body surface. The generated proximal relationship exhibits which region of the scene is in contact with the person. Furthermore, based on such synthesized proximity, we are able to effectively obtain expressive 3D human bodies that interact with the 3D scene naturally. Our perceptual study shows that PLACE significantly improves the state-of-the-art method, approaching the realism of real human-scene interaction. We believe our method makes an important step towards the fully automatic synthesis of realistic 3D human bodies in 3D scenes. The code and model are available for research at https://sanweiliti.github.io/PLACE/PLACE.html.">
<meta name="keywords" content="human-scene interaction; 3D scene; proximity learning; generating 3D human; deep learning; 3D vision; computer vision;">
<!--<link rel="author" href="http://wywu.github.io">-->

<!-- Fonts and stuff -->
<link href="./support/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./support/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./support/iconize.css">
<script async="" src="./support/prettify.js"></script>


</head>


<body>
  <div id="content">
    <div id="content-inner">

		<div class="section head">
	<h1><font size="5">PLACE: Proximity Learning of Articulation and Contact in 3D Environments</font></h1>

	<div class="authors">
		<a href="https://inf.ethz.ch/people/people-atoz/person-detail.MjQyOTY2.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html">Siwei Zhang</a><sup>1</sup>
		<a href="https://inf.ethz.ch/people/people-atoz/person-detail.MjcwNjU2.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html">Yan Zhang</a><sup>1</sup>
		<a href="https://www.is.mpg.de/person/qma">Qianli Ma</a><sup>2</sup>
	  	<a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a><sup>2</sup>&nbsp;
	  	<a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html">Siyu Tang</a><sup>1</sup>
	</div>

	<div class="affiliations">
	  	<sup>1</sup><a href="https://ethz.ch/en.html">ETH Zurich<br></a>
	  	<sup>2</sup><a href="https://is.mpg.de/">Max Planck Institute for Intelligent Systems<br></a>
	</div>

	<div class="venue">International Conference on 3D Vision (<a href="http://3dv2020.dgcv.nii.ac.jp/" target="_blank">3DV</a>) 2020 </div>

	<ul id="tabs">
		<li><a href="./PLACE.html" name="#tab1">PLACE</a></li>
	</ul>
	</div>


	

      
      <center><img src="./support/index.png" border="0" width="90%"></center>
      <div class="section abstract">
	<h2>Abstract</h2>
	<p>
High fidelity digital 3D environments have been proposed in recent years, however, it remains extremely challenging to automatically equip such environment with realistic human bodies. Existing work utilizes images, depth or semantic maps to represent the scene, and parametric human models to represent 3D bodies. While being straightforward, their generated human-scene interactions are often lack of naturalness and physical plausibility. Our key observation is that humans interact with the world through body-scene contact. To synthesize realistic human-scene interactions, it is essential to effectively represent the physical contact and proximity  between the body and the world. To that end, we propose a novel interaction generation method, named PLACE (Proximity Learning of Articulation and Contact in 3D Environments), which explicitly models the proximity between the human body and the 3D scene around it. Specifically, given a set of basis points on a scene mesh, we leverage a conditional variational autoencoder to synthesize the minimum distances from the basis points to the human body surface. The generated proximal relationship exhibits which region of the scene is in contact with the person. Furthermore, based on such synthesized proximity, we are able to effectively obtain expressive 3D human bodies that interact with the 3D scene naturally. Our perceptual study shows that PLACE significantly improves the state-of-the-art method, approaching the realism of real human-scene interaction. We believe our method makes an important step towards the fully automatic synthesis of realistic 3D human bodies in 3D scenes. The code and model are available for research at https://sanweiliti.github.io/PLACE/PLACE.html.
	</p>
      </div>
<div class="section downloads">
	<h2>Videos</h2><center>
		<iframe width="505" height="315" src="https://www.youtube.com/embed/zJ1hbtMHGrw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div></center>
		<!--<center>
	  <ul>
        <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=0K6ThmvYFgw" target="_blank" class="imageLink"><img src="./support/video.png"></a><br><a href="https://www.youtube.com/watch?v=0K6ThmvYFgw">Short video (2 min)</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" target="_blank" class="imageLink"><img src="./support/video.png"></a><br><a href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" target="_blank">Long video (7 min)</a>
		</div>
	      </li>
		  </ul>
	    </center>
	    </div>-->


      <div class="section downloads">
	<h2>Downloads</h2>
	<center>
	  <ul>
        <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/pdf/2008.05570.pdf" target="_blank" class="imageLink"><img src="./support/paper.png"></a><br><a href="https://arxiv.org/pdf/2008.05570.pdf">Paper</a>
		</div>
	      </li>
        <!--<li class="grid">-->
	      <!--<div class="griditem">-->
		<!--<a href="./support/ReenactGAN_Supplementary_Material.pdf" target="_blank" class="imageLink"><img src="./support/sup.png"></a><br><a href="./support/ReenactGAN_Supplementary_Material.pdf">Supplementary Material</a>-->
		<!--</div>-->
	      <!--</li>-->
	    <li class="grid">
	      <div class="griditem">
		<a href="https://drive.google.com/file/d/1zNFLpJ5m_IjwaEzyJXE7RIUSpZAXmk_q/view?usp=sharing" target="_blank" class="imageLink"><img src="./support/data.png"></a><br><a href="https://drive.google.com/file/d/1zNFLpJ5m_IjwaEzyJXE7RIUSpZAXmk_q/view?usp=sharing" target="_blank">Trained Models</a>
		</div>
	      </li>

	    <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/sanweiliti/PLACE" target="_blank" class="imageLink"><img src="./support/code.png"></a><br><a href="https://github.com/sanweiliti/PLACE" target="_blank">Code</a>
		</div>
	      </li>
	    </ul>
	    </center>
	    </div>
	    

<br>
 <div class="section list">
	<h2>Citation</h2>
	
	<div class="section bibtex">
	  <pre>@inproceedings{zhang2020place,
  title={PLACE: Proximity learning of articulation and contact in 3D environments},
  author={Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J and Tang, Siyu},
  booktitle={2020 International Conference on 3D Vision (3DV)},
  pages={642--651},
  year={2020},
  organization={IEEE}
}
	</pre>
	  </div>
      </div>

     <div class="section contact">
	<h2>Contact</h2>
		 For questions, please contact Siwei Zhang:<br><a href="mailto:siwei.zhang@inf.ethz.ch">siwei.zhang@inf.ethz.ch</a>
      </div>
    </div>
  </div>

</body></html>

